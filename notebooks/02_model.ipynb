{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import weaviate\n",
    "from ranx import Qrels, Run, evaluate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_corpus(train_df, itemid_docid_lookup):\n",
    "    \"\"\"Builds a list of TaggedDocument for training doc2vec model in gensim\"\"\"\n",
    "    # lookup the doc id for the item purchased\n",
    "    x = train_df.reset_index().set_index(\"item_purchase\")\\\n",
    "        .join(itemid_docid_lookup.set_index(\"item_id\"))\\\n",
    "        .set_index(\"session_id\")\n",
    "\n",
    "    # consolidate all item views into a single list by doc id\n",
    "    x = x.groupby([\"doc_id\"])\\\n",
    "        .agg(lambda x: np.concatenate(x).tolist())\\\n",
    "        .rename(columns={\"item_views\": \"tokens\"})\n",
    "\n",
    "    def f(x):\n",
    "        return gensim.models.doc2vec.TaggedDocument(x.values[0], [x.index.values[0]])\n",
    "\n",
    "    x = x.groupby(\"doc_id\")\\\n",
    "        .agg({\"tokens\": f})\\\n",
    "        .rename(columns={\"tokens\": \"tagged_doc\"})\n",
    "\n",
    "    return x[:][\"tagged_doc\"].tolist()\n",
    "\n",
    "\n",
    "def compute_mrr(df):\n",
    "    # this is the query id\n",
    "    assert df.index.name == \"session_id\"\n",
    "\n",
    "    # this is the ground truth i.e. item purchased\n",
    "    assert \"item_purchase\" in df.columns\n",
    "\n",
    "    # this is the items the model predicted will be bought given the session\n",
    "    assert \"predicted_purchase\" in df.columns\n",
    "\n",
    "    qrels_dict = {}\n",
    "    run_dict = {}\n",
    "\n",
    "    for query, correct_result in zip(df.index, df[\"item_purchase\"]):\n",
    "        qrels_dict[query] = {correct_result: 1}\n",
    "\n",
    "    for query, proposed_results in zip(df.index, df[\"predicted_purchase\"]):\n",
    "        run_dict[query] = {item_id: rank for rank,\n",
    "                           item_id in enumerate(proposed_results, 1)}\n",
    "\n",
    "    qrels = Qrels(qrels_dict)\n",
    "    run = Run(run_dict)\n",
    "\n",
    "    return evaluate(qrels, run, \"mrr\")\n",
    "\n",
    "\n",
    "def make_vectorizer_fn(model):\n",
    "    def f(query):\n",
    "        # query is a list of item views\n",
    "        v = model.infer_vector(query)\n",
    "        return v\n",
    "    return f\n",
    "\n",
    "\n",
    "def make_weaviate_inference_fn(client, topn=100):\n",
    "    def f(query_vector):\n",
    "        results = client.query.get(\"Item\", [\"itemid\"])\\\n",
    "            .with_limit(topn)\\\n",
    "            .with_near_vector({\n",
    "                \"vector\": query_vector\n",
    "            })\\\n",
    "            .do()\n",
    "\n",
    "        items = results[\"data\"][\"Get\"][\"Item\"]\n",
    "        return [item[\"itemid\"] for item in items]\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def make_gensim_inference_fn(model, itemid_docid_lookup, topn=100, ):\n",
    "    vectorizer_fn = make_vectorizer_fn(model)\n",
    "\n",
    "    def f(item_views):\n",
    "\n",
    "        v = vectorizer_fn(item_views)\n",
    "\n",
    "        predictions = model.dv.most_similar([v], topn=topn)\n",
    "\n",
    "        predicted_docids = [doc_id for (doc_id, _) in predictions]\n",
    "\n",
    "        predicted_itemids = itemid_docid_lookup.query(\n",
    "            \"doc_id in @predicted_docids\")[\"item_id\"].tolist()\n",
    "\n",
    "        return predicted_itemids\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def eval_prediction(actual_purchased_itemds, predicted_itemids):\n",
    "    if actual_purchased_itemds in predicted_itemids:\n",
    "        rank = (predicted_itemids.index(actual_purchased_itemds))\n",
    "    else:\n",
    "        rank = None\n",
    "\n",
    "    return rank\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"train_df.parquet\")\n",
    "valid_df = pd.read_parquet(\"valid_df.parquet\")\n",
    "test_df = pd.read_parquet(\"test_df.parquet\")\n",
    "\n",
    "itemid_docid_lookup = pd.read_parquet(\"itemid_docid_lookup.parquet\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore really long sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_session_length = 15\n",
    "\n",
    "is_short_session = train_df[\"item_views\"].apply(len) <= max_session_length\n",
    "\n",
    "train_df = train_df[is_short_session]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = build_training_corpus(train_df, itemid_docid_lookup)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(\n",
    "    vector_size=200, window=3, negative=25, sample=1e-3, min_count=20, epochs=80)\n",
    "model.build_vocab(train_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count,\n",
    "            epochs=model.epochs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the document vectors to weaviate. Since the data is so simple, we can use auto schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(\"http://weaviate:8080\")\n",
    "client.schema.delete_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batch(\n",
    "    batch_size=100,\n",
    "    dynamic=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "with client.batch as batch:\n",
    "\n",
    "    for docid in range(len(model.dv)):\n",
    "        vector = model.dv[docid]\n",
    "        itemid = itemid_docid_lookup.query(\n",
    "            \"doc_id == @docid\")[\"item_id\"].values[0]\n",
    "\n",
    "        batch.add_data_object(\n",
    "            data_object={\n",
    "                \"docid\": docid,\n",
    "                \"itemid\": itemid\n",
    "            },\n",
    "            class_name=\"Item\",\n",
    "            vector=vector\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the object count is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19020"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.dv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"nodes\":[{\"gitHash\":\"37d3b17\",\"name\":\"node1\",\"shards\":[{\"class\":\"Item\",\"name\":\"bjZQwoKvmmO1\",\"objectCount\":19020}],\"stats\":{\"objectCount\":19020,\"shardCount\":1},\"status\":\"HEALTHY\",\"version\":\"1.17.0\"}]}\n"
     ]
    }
   ],
   "source": [
    "!curl weaviate: 8080/v1/nodes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating on the training set should give decent results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "vectorizer_fn = make_vectorizer_fn(model)\n",
    "weaviate_inference_fn = make_weaviate_inference_fn(client, topn=n)\n",
    "gensim_inference_fn = make_gensim_inference_fn(\n",
    "    model, itemid_docid_lookup, topn=n)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time, evaluate only on a subset of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = train_df.sample(100_000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00023322698464674509"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df[\"predicted_purchase\"] = sample_df[\"item_views\"].apply(\n",
    "    lambda x: weaviate_inference_fn(vectorizer_fn(x)))\n",
    "compute_mrr(sample_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000328890214163869"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df[\"predicted_purchase\"] = sample_df[\"item_views\"].apply(\n",
    "    gensim_inference_fn)\n",
    "compute_mrr(sample_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are terrible! ðŸ˜­"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
